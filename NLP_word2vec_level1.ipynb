{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qiannzzu/VPONclassification/blob/plus7/NLP_word2vec_level1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding=utf-8  \n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn import feature_extraction  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import *\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "xI-Y7xs3EnlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ubp9ukjEBci"
      },
      "source": [
        "**Word2vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhMRetLhRVAx",
        "outputId": "ea3e7be7-849a-47be-f533-d7f79d20a4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu8gV8VfSN6_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "contents = pd.read_csv('/content/drive/MyDrive/word2vec/input/ckipTagger.txt', sep = '\\n', header=None).to_numpy().astype(str)\n",
        "labels = pd.read_csv('/content/drive/MyDrive/word2vec/input/labels.txt', sep = '\\n', header=None).to_numpy().astype(str)\n",
        "\n",
        "#print(contents.ravel()[0])\n",
        "#print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYMFrk_gS7dC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(contents.ravel(), labels, random_state=0, test_size=0.1)\n",
        "\n",
        "#print(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3rCKJtATOcL"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import word2vec\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/word2vec/input/y_360W_cbow_2D_100dim_2020v1.bin', unicode_errors='ignore', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SxyMXGop6KJ",
        "outputId": "8ce565b6-0f84-4b84-c604-712b1914dca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157700\n",
            "喜特麗 全 省 安裝 80 公分 雙 渦輪 增壓 全 不鏽鋼 燒焊 隱藏式 排油煙機 抽油煙機 JT 1835 M 喜特麗\n"
          ]
        }
      ],
      "source": [
        "print(len(x_train))\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#使用Cross Entropy(又稱logloss)是最常使用於分類問題的損失函數(loss functions)\n",
        "\n",
        "def multiclass_logloss(actual, predicted, eps=1e-15): #在kaggle上常用於計算分類問題之Loss func\n",
        "\n",
        "    \"\"\"對數損失度量（Logarithmic Loss  Metric）的多分類版本。\n",
        "\n",
        "    :param actual: 包含actual target classes的陣列\n",
        "\n",
        "    :param predicted: 分類預測結果矩陣, 每個類別都有一個概率\n",
        "\n",
        "    \"\"\"\n",
        "    actual = actual.astype(int)\n",
        "    # Convert 'actual' to a binary array if it's not already:\n",
        "\n",
        "    if len(actual.shape) == 1:\n",
        "\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "\n",
        "        for i, val in enumerate(actual):\n",
        "\n",
        "            actual2[i, val] = 1\n",
        "\n",
        "        actual = actual2\n",
        "\n",
        "    clip = np.clip(predicted, eps, 1 - eps)\n",
        "    rows = actual.shape[0]\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "   \n",
        "    return -1.0 / rows * vsota"
      ],
      "metadata": {
        "id": "nXi3YXDlEsc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C92m492l1Uh",
        "outputId": "70b4c138-431f-4f67-afd8-898a8f8ce44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def sent2vec(s):\n",
        "  words = word_tokenize(s) #斷詞功能 ex\"At eight o'clock on Thursday morning.\"->['At','eight',\"o'clock\",...]\n",
        "  words = [w for w in words if w.isalpha()]\n",
        "  if len(words) > 30:\n",
        "    words = words[0:30]\n",
        "  if len(words) < 30:\n",
        "    words.extend([\"0\"]*(30-len(words)))\n",
        "    \n",
        "    \n",
        "  M = []\n",
        "\n",
        "  for w in words:\n",
        "\n",
        "      try:\n",
        "           M.append(model[str(w)])\n",
        "      except:\n",
        "           M.append(model[str(0)])\n",
        "\n",
        "  M = np.array(M)\n",
        " \n",
        "  if type(M) != np.ndarray:\n",
        "      # print(\"\\n!\")\n",
        "      return np.zeros(30,100)\n",
        "  if M.shape != (30,100):\n",
        "    print(M.shape)\n",
        "  return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTybteXCiPfs",
        "outputId": "59397775-b4ee-4479-ab04-d7165966f1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157700/157700 [01:08<00:00, 2296.64it/s]\n",
            "100%|██████████| 17523/17523 [00:03<00:00, 4515.62it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm #顯示進度條\n",
        "xtrain_w2v  = [sent2vec(x) for x in tqdm(x_train)]\n",
        "# xtrain_w2v  = sent2vec(xtrain[2])\n",
        "xtest_w2v  = [sent2vec(x) for x in tqdm(x_test)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aacdr1WumKbD",
        "outputId": "92badc53-fb3d-41e0-c837-98bc799a4a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(157700, 30, 100)\n",
            "<class 'list'>\n",
            "[array([[  0.5966747,   1.9254155,   2.9950552, ...,  -6.125816 ,\n",
            "          7.4699855,   2.010785 ],\n",
            "       [  1.8764926,   6.59889  ,  -8.513383 , ...,   1.491173 ,\n",
            "         15.018477 ,  -2.2956195],\n",
            "       [ 11.532459 ,   9.224527 ,   6.3856153, ...,  -1.9438661,\n",
            "         -4.4415846, -10.076724 ],\n",
            "       ...,\n",
            "       [ -2.3035665,  -4.41802  ,  -7.8560004, ...,  -2.469303 ,\n",
            "          4.8267155,   5.79327  ],\n",
            "       [ -2.3035665,  -4.41802  ,  -7.8560004, ...,  -2.469303 ,\n",
            "          4.8267155,   5.79327  ],\n",
            "       [ -2.3035665,  -4.41802  ,  -7.8560004, ...,  -2.469303 ,\n",
            "          4.8267155,   5.79327  ]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "print(np.shape(xtrain_w2v))\n",
        "print(type(xtrain_w2v))\n",
        "print(xtrain_w2v[:1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
        "xtrain_w2v = np.array(xtrain_w2v)\n",
        "xtrain_w2v = xtrain_w2v.reshape(xtrain_w2v.shape[0], -1)\n",
        "xtest_w2v = np.array(xtest_w2v)\n",
        "#print(np.shape(xtest_w2v))\n",
        "xtest_w2v = xtest_w2v.reshape(xtest_w2v.shape[0], -1)\n",
        "y_train = y_train.ravel()\n",
        "y_test = y_test.ravel()\n",
        "print(np.shape(xtrain_w2v))\n",
        "print(np.shape(xtest_w2v))\n",
        "print(np.shape(y_test))"
      ],
      "metadata": {
        "id": "c_twQGtyuBhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6df66f-32e5-45d0-f64b-7778817f894f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(157700, 3000)\n",
            "(17523, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf03NWI7UKLQ",
        "outputId": "b242f002-1a1d-4d86-9761-211d885e1ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19 30 10 ... 19  2 31]\n"
          ]
        }
      ],
      "source": [
        "#import xgboost as xgb\n",
        "#xtrain_w2v = np.array(xtrain_w2v)\n",
        "#xtrain_w2v = xtrain_w2v.reshape(xtrain_w2v.shape[0], -1)\n",
        "\n",
        "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
        "\n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False, tree_method='gpu_hist', gpu_id=0) #, tree_method='gpu_hist', gpu_id=0\n",
        "\n",
        "clf.fit(xtrain_w2v, y_train)\n",
        "\n",
        "predictions = clf.predict_proba(xtest_w2v)\n",
        "\n",
        "arr = np.array(y_test).astype(int) # converting list to array\n",
        "print(arr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(predictions.shape)\n",
        "#print(predictions[:5])\n",
        "#print(y_test.shape)\n",
        "#print(y_train.shape)\n",
        "print (\"word2vec+xgboost logloss: %0.3f \" % multiclass_logloss(arr, predictions))\n",
        "\n",
        "predictions = clf.predict(xtest_w2v)\n",
        "print('xgboost模型的準確度:{}'.format(accuracy_score(y_test,predictions)))\n",
        "\n",
        "#word2vec+xgboost logloss: 8.866 \n",
        "#xgboost模型的準確度:0.8696570221993951"
      ],
      "metadata": {
        "id": "CwM6qcVx9d8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7eb8e6-ad2c-4e6b-db78-ad7e10bab92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2vec+xgboost logloss: 8.866 \n",
            "xgboost模型的準確度:0.8696570221993951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#w2v+LR 100dim+max_iter 400\n",
        "#模型的準確度:0.7571762826000115\n",
        "#LR_w2v logloss: 15.117 \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LR_w2v = LogisticRegression(solver='lbfgs', max_iter=400) #400\n",
        "LR_w2v.fit(xtrain_w2v, y_train)"
      ],
      "metadata": {
        "id": "-RO4ddFZGnB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f189c73-4896-4c9f-872b-d8a248ce6638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=400)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = LR_w2v.predict_proba(xtest_w2v) \n",
        "#print(predictions.shape)\n",
        "#print(predictions[:5])\n",
        "#print(y_test.shape)\n",
        "print('LR_w2v模型的準確度:{}'.format(LR_w2v.score(xtest_w2v, y_test)))\n",
        "arr = np.array(y_test) # converting list to array\n",
        "print (\"LR_w2v logloss: %0.3f \" % multiclass_logloss(arr, predictions))"
      ],
      "metadata": {
        "id": "2_NCJJAEY7YU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c639ac-e14d-43b9-c2a8-a65e540a812e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR_w2v模型的準確度:0.7571762826000115\n",
            "LR_w2v logloss: 15.117 \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_word2vec_level1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}