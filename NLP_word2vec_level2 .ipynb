{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_word2vec_level2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#encoding=utf-8  \n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn import feature_extraction  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#導入資料集\n",
        "item = pd.read_csv('online_shopping_items_partial.csv')\n",
        "#新增自定義詞典和停用詞典\n",
        "# jieba.load_userdict(\"itemBrand.txt\") #自定義詞典設為所有brand name 但造成accuracy下降 故不採用\n",
        "stop_list = pd.read_csv('hit_stopword.txt',\n",
        "                        engine='python',\n",
        "                        encoding='UTF-8',\n",
        "                        error_bad_lines=False,\n",
        "                        delimiter=\"\\n\",\n",
        "                        names=['t'])['t'].tolist()\n",
        "\n",
        "\n",
        "#中文分詞函式\n",
        "def txt_cut(juzi):\n",
        "    return [w for w in jieba.lcut(juzi) if w not in stop_list]\n",
        "\n",
        "#寫入分詞結果\n",
        "import csv\n",
        "fw = open('fenci_data.csv', \"a+\", newline = '',encoding = 'gb18030')\n",
        "writer = csv.writer(fw)  \n",
        "writer.writerow(['item_name','item_category'])\n",
        "\n",
        "#label encoding\n",
        "\n",
        "lbl = preprocessing.LabelEncoder() #將文字進行資料前處理->Label encoding\n",
        "item['item_category'] = lbl.fit_transform(item['item_category']) #item_category欄位做Label encoding\n",
        "item['item_category2'] = lbl.fit_transform(item['item_category2'])\n",
        "# print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_7vrqqm1mOc",
        "outputId": "76a0ed21-9147-4dc2-d007-b89e7981fa5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "Skipping line 219: unexpected end of data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#使用Cross Entropy(又稱logloss)是最常使用於分類問題的損失函數(loss functions)\n",
        "\n",
        "def multiclass_logloss(actual, predicted, eps=1e-15): #在kaggle上常用於計算分類問題之Loss func\n",
        "\n",
        "    \"\"\"對數損失度量（Logarithmic Loss  Metric）的多分類版本。\n",
        "\n",
        "    :param actual: 包含actual target classes的陣列\n",
        "\n",
        "    :param predicted: 分類預測結果矩陣, 每個類別都有一個概率\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert 'actual' to a binary array if it's not already:\n",
        "\n",
        "    if len(actual.shape) == 1:\n",
        "\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "\n",
        "        for i, val in enumerate(actual):\n",
        "\n",
        "            actual2[i, val] = 1\n",
        "\n",
        "        actual = actual2\n",
        "\n",
        "\n",
        "\n",
        "    clip = np.clip(predicted, eps, 1 - eps)\n",
        "\n",
        "    rows = actual.shape[0]\n",
        "\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "\n",
        "    return -1.0 / rows * vsota"
      ],
      "metadata": {
        "id": "n18QqTgiGJfH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#文字資料預處理\n",
        "import re \n",
        " \n",
        "# # 只提取出中文出來 準確率較低 捨棄\n",
        "# for i in range(0,len(item)): \n",
        "#   new_data = re.findall('[\\u4e00-\\u9fa5]+', item['item_name'][i], re.S) \n",
        "#   item['item_name'][i] = \"\".join(new_data) \n",
        "\n",
        "# 去除數字 準確度較高\n",
        "for i in range(0,len(item)):\n",
        "  input_str = item['item_name'][i]\n",
        "  item['item_name'][i] = re.sub(r'\\d+', '', input_str) \n",
        "  # print(result) \n",
        "print(item['item_name'][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OgtpzHb2FAv",
        "outputId": "7e30ab5a-e2dc-412f-813c-214b5fc1d282"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0           【美國Mega】大維錠狀食品-純素 錠(入)\n",
            "1     【良醇酵素】樟芝天然綜合水果益生菌酵素發酵液(mlx瓶)\n",
            "2    【桂格】養氣人蔘禮盒ml×入(超夯伴手禮 送禮體面又健康)\n",
            "3               【以馬內利】雪麗維他命C錠(顆/瓶)\n",
            "4      【Sundown 日落恩賜】勇健鈣鎂鋅加強錠錠(瓶組)\n",
            "Name: item_name, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#斷詞\n",
        "# 使用csv.DictReader讀取檔案中的資訊\n",
        "labels2 = []\n",
        "contents = []\n",
        "outputxt = open('corpusSegDone.txt', 'w', encoding='utf-8')\n",
        "for i in range(0,len(item)):\n",
        "    # res1 = item['item_category'][i]\n",
        "    res2 = item['item_category2'][i]\n",
        "    # labels.append(res)\n",
        "    labels2.append(res2) # level2 的label\n",
        "    content = item['item_name'][i]\n",
        "    a=str(content)\n",
        "    seglist = txt_cut(a) #含去除stoplist\n",
        "    output = ' '.join(list(seglist))   #原本每列都被分隔['','','','']->改由空格拼接 :Dr AV NP 三洋 洗衣 機專用 濾網 超值 四入 組\n",
        "    #加上品牌之column做為特徵\n",
        "    output = output+' '+item['item_brand'][i]\n",
        "    #加上價錢之column做為特徵(使level1之accuracy降低 捨棄不看)\n",
        "    # output = output+' '+str(item['price'][i])\n",
        "    output = output+' '+str(item['item_category'][i])\n",
        "    contents.append(output)\n",
        "    outputxt.write(output)\n",
        "    outputxt.write('\\n')\n",
        "    \n",
        "    # 檔案寫入fenci.csv\n",
        "    # tlist = []\n",
        "    # tlist.append(output)\n",
        "    # # tlist.append(res)\n",
        "    # writer.writerow(tlist)\n",
        "outputxt.close()\n"
      ],
      "metadata": {
        "id": "NOng8zlZ2Kg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036c1a01-538b-4610-b569-c483d03f110e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.828 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(contents[195:201])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de8Y9NCtoWLi",
        "outputId": "cf1a7489-6c13-4b0b-9b92-e80d2e20ed86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['金 蔘 韓國 高麗人 蔘 精華液 禮盒 ml 瓶   共盒 金蔘 0', '珍果 諾麗康 濃縮 綜合汁 mlx 瓶 珍果生技 0', '聿 健 芝麻 EX 夜夜 暝 膠囊粒 盒 入組 聿健 0', '葡萄 王 孅 益 薑 黃 粒 ％ 高 含量 薑 黃   代謝 甩 囤積 葡萄王 0', '葡萄 王 認證 靈芝 王粒 X 瓶     粒 國家 調節 免疫力 健康 食品 認證   靈芝 多醣 % 葡萄王 0', 'Panasonic   國際牌 KG 變頻 直立式 洗衣 機 NA VLM L Panasonic 國際牌 9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tfidf**\n"
      ],
      "metadata": {
        "id": "5_I1MK9i5dFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------資料處理--------------------------------\n",
        "#將文字中的詞語轉換為詞頻矩陣 矩陣元素a[i][j] 表示j詞在i類文字下的詞頻\n",
        "vectorizer = CountVectorizer(min_df=5) #min_df=5: 如果某個詞的document frequence小於min_df，則這個詞不會被當作關鍵詞。\n",
        "#該類會統計每個詞語的tf-idf權值\n",
        "transformer = TfidfTransformer()\n",
        "\n",
        "#第一個fit_transform是計算tf-idf 第二個fit_transform是將文字轉為詞頻矩陣\n",
        "tfidf = transformer.fit_transform(vectorizer.fit_transform(contents))\n",
        "\n",
        "# 獲取詞袋模型中的所有詞語(可用於查看有哪些詞)\n",
        "# word = vectorizer.get_feature_names()\n",
        "# print(\"單詞數量:\", len(word))\n",
        "\n",
        "#將tf-idf矩陣抽取出來，元素w[i][j]表示j詞在i類文字中的tf-idf權重\n",
        "#X = tfidf.toarray()\n",
        "X = coo_matrix(tfidf, dtype=np.float32).toarray() #稀疏矩陣 注意float\n",
        "\n",
        "#----------------------------------資料分配--------------------------------\n",
        "#資料集做split (for tfidf)\n",
        "#使用 train_test_split 分割 X y 列表 train/test: 0.8/0.2\n",
        "X_train, X_test,\\\n",
        "y_train, y_test = train_test_split(X, labels2, \n",
        "                   test_size=0.2, \n",
        "                   random_state=1)"
      ],
      "metadata": {
        "id": "Mrgq8snn2UwP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tfidf+LogisticRegression**"
      ],
      "metadata": {
        "id": "YgC7crmT6e__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 邏輯迴歸分類方法模型\n",
        "# LR = LogisticRegression(solver='lbfgs', max_iter=400)\n",
        "# LR.fit(X_train, y_train)\n",
        "# print('Tfidf+LR模型 準確度:{}'.format(LR.score(X_test, y_test)))\n",
        "# predictions = LR.predict_proba(X_test) \n",
        "# arr = np.array(y_test) # converting list to array\n",
        "# print (\"Tfidf+LR logloss: %0.3f \" % multiclass_logloss(arr, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2xsZIv_6Q5A",
        "outputId": "38053a9d-66de-4e02-87c9-035b01d60985"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tfidf+LR模型 準確度:0.8921450151057402\n",
            "Tfidf+LR logloss: 0.743 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tfidf+XGBoost**"
      ],
      "metadata": {
        "id": "JDzdddvdEHYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install xgboost"
      ],
      "metadata": {
        "id": "FKbIG3d6ITbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost 這個模型很慢 會跑很久QQ\n",
        "import xgboost as xgb\n",
        "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
        "\n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict_proba(X_test)\n",
        "\n",
        "arr = np.array(y_test) # converting list to array\n",
        "\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(arr, predictions))\n",
        "\n",
        "print('xgboost模型的準確度:{}'.format(clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "CfTdc6QUHz2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2vec**"
      ],
      "metadata": {
        "id": "1Ubp9ukjEBci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import gensim\n",
        "# from gensim.models import word2vec\n",
        "\n",
        "# # model = gensim.models.Word2Vec(contents, size=100, min_count=5) #分詞結果轉為word2vec詞向量模型（100維）\n",
        "# sentences = word2vec.LineSentence(\"corpusSegDone.txt\")\n",
        "# model = word2vec.Word2Vec(sentences, size=100)\n",
        "# embeddings_index = dict(zip(model.wv.index2word, model.wv.vectors ))  #Word2Vec模型中的词汇表存储在model.wv.index2word 特征向量存储在叫做syn0的numpy数组\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-HWZTDlqgfM",
        "outputId": "51c3f8b1-5ee5-4ca2-9498-0b9b556277e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10936 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #資料集做split (for word2vec)\n",
        "# xtrain, xvalid, ytrain, yvalid = train_test_split(contents, labels2,\n",
        "#                                                   random_state=1,\n",
        "#                                                   test_size=0.2, shuffle=True)\n",
        "# # #L2 資料集做split (for word2vec)\n",
        "# # xtrain_l2, xvalid_l2, ytrain_l2, yvalid_l2 = train_test_split(xvalid, yvalid,\n",
        "# #                                                   random_state=1,\n",
        "# #                                                   test_size=0.2, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "voFju2ZPTntI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# from nltk import word_tokenize\n",
        "\n",
        "# #該函式會將語句轉化為一個標準化的向量（Normalized Vector）\n",
        "# def sent2vec(s):\n",
        "#     # print (s)\n",
        "#     words = word_tokenize(s) #斷詞功能 ex\"At eight o'clock on Thursday morning.\"->['At','eight',\"o'clock\",...]\n",
        "#     words = [w for w in words if w.isalpha()]\n",
        "#     M = []\n",
        "\n",
        "#     for w in words:\n",
        "\n",
        "#         try:\n",
        "#             # print(embeddings_index[w].shape)\n",
        "#             M.append(embeddings_index[w])\n",
        "\n",
        "#         except:\n",
        "#             continue\n",
        "\n",
        "#     M = np.array(M)\n",
        "#     v = M.sum(axis=0)\n",
        "#     # print(\"\\nv.shape:\",v.shape)\n",
        "\n",
        "#     if type(v) != np.ndarray:\n",
        "#         # print(\"\\n!\")\n",
        "#         return np.zeros(100)\n",
        "\n",
        "#     return v / np.sqrt((v ** 2).sum())\n",
        "# # contents向量化\n",
        "# from tqdm import tqdm #顯示進度條\n",
        "# xtrain_w2v  = [sent2vec(x) for x in tqdm(xtrain)]\n",
        "# # xtrain_w2v  = sent2vec(xtrain[2])\n",
        "# xvalid_w2v  = [sent2vec(x) for x in tqdm(xvalid)]\n"
      ],
      "metadata": {
        "id": "Mz7b-uzMqr4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901723b7-3ff6-467a-b6cb-66c2e9059ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26478/26478 [00:05<00:00, 5211.94it/s]\n",
            "100%|██████████| 6620/6620 [00:02<00:00, 2545.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2vec+LogisticRegression**"
      ],
      "metadata": {
        "id": "d-FzwzJh7E2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #模型的準確度:0.5811023622047244?? 全部data之accuracy: 0.7408263425212578\n",
        "# LR_w2v = LogisticRegression(solver='lbfgs', max_iter=400)\n",
        "# LR_w2v.fit(xtrain_w2v, ytrain)\n",
        "# predictions = LR_w2v.predict_proba(xvalid_w2v) \n",
        "# print('LR_w2v模型的準確度:{}'.format(LR_w2v.score(xvalid_w2v, yvalid)))\n",
        "# arr = np.array(yvalid) # converting list to array\n",
        "# print (\"LR_w2v logloss: %0.3f \" % multiclass_logloss(arr, predictions))\n",
        "\n"
      ],
      "metadata": {
        "id": "1ikV44TmujMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f714f81a-893f-492c-9b55-3d9203317858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR_w2v模型的準確度:0.5648036253776435\n",
            "LR_w2v logloss: 1.667 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gguHrqrCvSQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Level2** - LR_w2v"
      ],
      "metadata": {
        "id": "6y3p83cJELTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions_L2 = LR_w2v.predict(xvalid_w2v) \n",
        "print(predictions_L2[:5])\n",
        "print(yvalid[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVMUPEbcEKH9",
        "outputId": "8c59cc67-7bf1-47e3-cc5d-bd5da3722352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 89 332 302 104 367]\n",
            "[318, 332, 166, 171, 367]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #斷詞\n",
        "# # 使用csv.DictReader讀取檔案中的資訊\n",
        "# labels = []\n",
        "# contents = []\n",
        "# outputxt = open('corpusSeg2.txt', 'w', encoding='utf-8')\n",
        "# for i in range(0,len(item)):\n",
        "#     res1 = item['item_category'][i]\n",
        "#     res2 = item['item_category2'][i]\n",
        "#     labels.append(res)\n",
        "#     labels2.append(res2) # level2 的label\n",
        "#     content = item['item_name'][i]\n",
        "#     a=str(content)\n",
        "#     seglist = txt_cut(a) #含去除stoplist\n",
        "#     output = ' '.join(list(seglist))   #原本每列都被分隔['','','','']->改由空格拼接 :Dr AV NP 三洋 洗衣 機專用 濾網 超值 四入 組\n",
        "#     #加上品牌之column做為特徵\n",
        "#     output = output+' '+item['item_brand'][i]\n",
        "#     #加上價錢之column做為特徵(使level1之accuracy降低 捨棄不看)\n",
        "#     # output = output+' '+str(item['price'][i])\n",
        "\n",
        "#     contents.append(output)\n",
        "#     outputxt.write(output)\n",
        "#     outputxt.write('\\n')\n",
        "    \n",
        "#     #檔案寫入fenci.csv\n",
        "#     tlist = []\n",
        "#     tlist.append(output)\n",
        "#     tlist.append(res)\n",
        "#     writer.writerow(tlist)\n",
        "# outputxt.close()"
      ],
      "metadata": {
        "id": "BMTWG3ZccCO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #word2vec\n",
        "# # model = gensim.models.Word2Vec(contents, size=100, min_count=5) #分詞結果轉為word2vec詞向量模型（100維）\n",
        "# sentences = word2vec.LineSentence(\"corpusSegDone.txt\")\n",
        "# model = word2vec.Word2Vec(sentences, size=100)\n",
        "# embeddings_index = dict(zip(model.wv.index2word, model.wv.vectors ))  #Word2Vec模型中的词汇表存储在model.wv.index2word 特征向量存储在叫做syn0的numpy数组\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "6dAl-c8lf4MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import multiprocessing\n",
        "# import sys\n",
        "# import xgboost as xgb\n"
      ],
      "metadata": {
        "id": "jBRLaoiGtTQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2vec+XGBoost**"
      ],
      "metadata": {
        "id": "drbZjAyZ7OM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xtrain_w2v = np.array(xtrain_w2v)\n",
        "# clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
        "\n",
        "#                         subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
        "\n",
        "# clf.fit(xtrain_w2v, ytrain)\n",
        "\n",
        "# predictions = clf.predict_proba(xvalid_w2v)\n",
        "# arr = np.array(yvalid) # converting list to array\n",
        "\n",
        "# print (\"word2vec+xgboost logloss: %0.3f \" % multiclass_logloss(arr, predictions))\n",
        "\n",
        "# predictions = clf.predict(xvalid_w2v)\n",
        "# print('xgboost模型的準確度:{}'.format(accuracy_score(yvalid,predictions)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCAcZkSguL5W",
        "outputId": "954635a2-5201-4d9e-a6aa-7091f91390e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2vec+xgboost logloss: 1.063 \n",
            "xgboost模型的準確度:0.7237160120845921\n"
          ]
        }
      ]
    }
  ]
}